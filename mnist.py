# -*- coding: utf-8 -*-
"""Mnist Simulation (strcutured).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ukDf8SqUJn-ffNvL_02OTjZacLLZtgC-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from scipy.optimize import Bounds
from scipy.optimize import minimize
import itertools
from tqdm import tqdm
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
# %matplotlib inline
class AttrDict(dict):
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__

"""Load Mnist"""

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import check_random_state

class Mnist():
  def __init__(self, batch_size):
    self.batch_size = batch_size
    self.train_ind = 0
    self.test_ind = 0
    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)

    # shuffle
    random_state = check_random_state(0)
    permutation = random_state.permutation(X.shape[0])
    X = X[permutation]
    y = y[permutation]
    X = X.reshape((X.shape[0], -1))

    # train/test split
    X_train, X_test, self.y_train, self.y_test = train_test_split(
        X, y, train_size=60000, test_size=10000)

    # normaloze and scale
    scaler = StandardScaler()
    self.X_train = scaler.fit_transform(X_train)
    self.X_test = scaler.transform(X_test)
    
  def get_data(self, is_train = True):
    ind = 0
    if is_train:
      source = self.X_train
      target = self.y_train
      ind = self.train_ind
    else:
      source = self.X_test
      target = self.y_test
      ind = self.test_ind
        
    while ind < source.shape[0] - 1:

      bb_size = min(self.batch_size, source.shape[0] - ind)

      data = source[ind: ind + bb_size]
      label = target[ind: ind + bb_size]

      ind += bb_size
      yield (data, label)

mnist = Mnist(1)

"""Constants, Inputs And Weights"""

# Constants
config = AttrDict()

# input dim
input_dims = [28, 28, 1]
config.input_dims = np.array(input_dims)

# network param
config.nb_strides = [2]
config.nb_layers = len(config.nb_strides)

config.nb_radius = [4]
assert(len(config.nb_radius) == config.nb_layers)

config.nps = [4]
assert(len(config.nps) == config.nb_layers)

config.nb_units = [config.input_dims]
next_num_units = config.input_dims
for i in range(config.nb_layers):
  next_num_units = (next_num_units/config.nb_strides[i]).astype(int)
  next_num_units[-1] = config.nps[i]
  assert(next_num_units.all() > 0) # must be positive
  config.nb_units.append(next_num_units)

config.nb_linear_units = []
for i in range(config.nb_layers + 1):
  config.nb_linear_units.append(int(np.prod(config.nb_units[i])))

# firing rate bounds
config.lower_bound = 0
config.upper_bound = 2

# opt param
config.lambda_1 = 0.1
config.lambda_2 = 1.0

# analog param
config.dt = 1.0

# spiking param
config.dt_s = 0.001
if config.dt_s >= 1/config.upper_bound:
  config.dt_r = 0
else:
  config.dt_r = (1/config.upper_bound)/config.dt_s
config.v_r = 0

# learning param
config.lr = 0.01

"""Weight class"""

class Weights():
  def __init__(self, config, layer_ind):
    self.config = config
    assert(layer_ind < config.nb_layers)
    # Weights Init

    self.W = np.random.uniform(0, 0.1, size = (config.nb_linear_units[layer_ind + 1], config.nb_linear_units[layer_ind]))
    self.W_mask = self.create_w_mask(config.nb_units[layer_ind], config.nb_units[layer_ind+1], config.nb_strides[layer_ind], config.nb_radius[layer_ind])
    
    _l_half = np.random.uniform(0, 0.1/np.sqrt(config.nb_linear_units[layer_ind]), size = (config.nb_linear_units[layer_ind], 1))
    self.L_mask = self.create_w_mask(config.nb_units[layer_ind+1], config.nb_units[layer_ind+1], config.nb_strides[layer_ind], 0, feedforward = False)
    self.L = _l_half @ _l_half.T
    
  def create_w_mask(self, input_dims, output_dims, stride, radius, feedforward = True):
    """
		compute once, so it may not be highly optimized.
		the mask is the same for all channel pairs
		"""
    in_nps = input_dims[-1]
    out_nps = output_dims[-1]
    stride_padding = stride/2
    fea_dim_ind = len(input_dims)-1
    input_linear_dim = int(np.prod(input_dims))
    output_linear_dim = int(np.prod(output_dims))

    single_mask_dims = np.concatenate([output_dims[:-1], input_dims[:-1]])
    c_W = np.zeros(list(single_mask_dims))

    # create indices matrix for easy dist calculation
    input_indices = []
    for dim_ind, dim in np.ndenumerate(input_dims[:-1]):
      repeat_vec = (np.ones(len(input_dims)) * input_dims).astype(int)
      repeat_vec[dim_ind] = 1
      repeat_vec[-1] = 1

      view_vec = np.ones(len(input_dims)).astype(int)
      view_vec[dim_ind] = dim

      indices_per_dim = np.tile(np.arange(dim).reshape(list(view_vec)), (list(repeat_vec)))
      input_indices.append(indices_per_dim)

    # Hugo Parameter -> 0.5
    if feedforward:
      input_indices_tensor = np.concatenate(input_indices, axis = fea_dim_ind) + 0.5
    else:
      input_indices_tensor = np.concatenate(input_indices, axis = fea_dim_ind)

    # calculate C for one channel pair
    for idx in itertools.product(*[range(s) for s in output_dims[:-1]]):
      if feedforward:
        center = np.array(idx) * stride + stride_padding
      else:
        center = np.array(idx)
      dist_mat = np.linalg.norm(input_indices_tensor-center, ord = 2, axis = fea_dim_ind)
      c_W[idx] = (dist_mat <= radius)

    # replicate C for all channel pairs
    nps_repeat_vec = np.ones(len(input_dims) + len(output_dims)).astype(int)
    nps_repeat_vec[len(output_dims)-1] = out_nps
    nps_repeat_vec[len(input_dims)-1 + len(output_dims)] = in_nps

    nps_view_vec = np.concatenate([output_dims, input_dims]).astype(int)
    nps_view_vec[len(output_dims)-1] = 1
    nps_view_vec[len(input_dims)-1 + len(output_dims)] = 1

    c_W_tile = np.tile(c_W.reshape(list(nps_view_vec)),(list(nps_repeat_vec)))
    return c_W_tile.reshape([output_linear_dim, input_linear_dim])
  
  def fetch_weights_and_masks(self):
    return self.W, self.W_mask, self.L, self.L_mask
  
  def fetch_masked_weights(self):
    return self.W * self.W_mask, self.L * self.L_mask
  
  def update(self, r_prev, r_cur):
    update_step = self.config.lr
    
    dW = update_step * (r_cur.T @ r_prev - self.W * self.W_mask)
    dL = update_step / 2 * (r_cur.T @ r_cur - self.L)
    
    self.W += dW
    self.L += dL

"""Loss Function"""

# Define SNN layer

class SNN_Layer():
  def __init__(self, config, layer_ind, Weights):
    self.dt_s = config.dt_s
    self.dt_r = config.dt_r
    self.v_r = config.v_r
    self.lambda_1 = config.lambda_1
    self.nb_units = config.nb_linear_units[layer_ind + 1]
    self.v_f_adjustment = 0.0
    
    W, L = weights.fetch_masked_weights()
    self.W = W
    self.L = L
    self.L_diag = np.diag(L)
    self.L_hat = self.L - np.diag(np.diag(L)) + self.v_f_adjustment*np.eye(self.nb_units)
    
    # The firing rate for neuron 2 should be lambda_2 + L_ii
    self.v_f = config.lambda_2 + self.L_diag - self.v_f_adjustment
    
    if (self.v_f < 0).any():
      raise ValueError('Choose a larger lambda_2')
    
    
    # not meant to be used before calling init_v_i()
    self.i_s = None
    self.v_s = None
    self.delta = None
    self.spike_count = None
    self.time_tick = None
    self.refactory_clock = None
    self.y = None
  
  def init_v_i(self, x):
    batch_size = x.shape[0]
    self.i_s = x @ self.W.T - self.lambda_1
    self.v_s = np.ones([batch_size, self.nb_units]) * self.v_r
    self.delta = np.zeros([batch_size, self.nb_units])
    self.spike_count = np.zeros([batch_size, self.nb_units])
    self.time_tick = 0
    self.refactory_clock = np.ones([batch_size, self.nb_units]) * self.dt_r
    self.y = np.zeros([batch_size, self.nb_units])
  
  def _refactory_counting(self):
    self.refactory_clock -= 1
    self.refactory_clock *= (self.refactory_clock > 0)
    
  def _active_mask(self):
    return self.refactory_clock <= 0
  
  def _integ_mask(self):
    integrate_mask = self.v_s <= self.v_f
    return integrate_mask
  
  def _fire(self):
    fire_mask = (self.v_s >= self.v_f) * self._active_mask()
    self.delta = fire_mask * 1
    self.spike_count += self.delta
    self.v_s[fire_mask] = self.v_r
    self.refactory_clock[fire_mask] = self.dt_r

  def run(self, x):
    for _ in range(50000):
      self.time_tick += 1

      mask = self._integ_mask()
      self._refactory_counting()
      
      # spike dynamics
      di_s = - self.i_s +  x @ self.W.T - self.lambda_1 - self.delta @ self.L_hat.T 
      self.i_s += di_s * self.dt_s * mask
      self.v_s += self.i_s * self.dt_s * mask
      
      self._fire()
      
      self.y = self.spike_count/(self.time_tick*self.dt_s)
      
      if self.time_tick % 100000== 0:
        print(self.y)
    
    return self.y
  
  def update_weights(self, x):
    weights.update(x, self.y)
    W, L = weights.fetch_masked_weights()
    self.W = W
    self.L = L
    self.L_diag = np.diag(L)
    self.L_hat = self.L - np.diag(np.diag(L)) + self.v_f_adjustment*np.eye(self.nb_units)

# Spike with Refactory Period

# input
layer_ind = 0 # Single Layer for now
weights = Weights(config, layer_ind)
snn_layer = SNN_Layer(config, layer_ind, weights)
  
for _ in tqdm(range(2)):
  iterator = mnist.get_data()
  for data, label in iterator:
    snn_layer.init_v_i(data)
    y_s = snn_layer.run(data)
    snn_layer.update_weights(data)

"""Classification"""

fea_dim = config.nb_linear_units[-1]
train_X = np.zeros((60000, fea_dim))
train_Y = np.zeros(60000)
test_X = np.zeros((10000, fea_dim))
test_Y = np.zeros(10000)

# get training data
start_save_idx = 0
iterator = mnist.get_data()
for idx, (data, label) in enumerate(tqdm(iterator)):
  batch_size = data.shape[0] # input batch size

  snn_layer.init_v_i(data)
  fea = snn_layer.run(data)

  train_X[start_save_idx: start_save_idx + batch_size, :] = fea
  train_Y[start_save_idx: start_save_idx + batch_size] = label
  start_save_idx += batch_size

print("finishing getting training data ...")

# get test data
start_save_idx = 0
iterator = mnist.get_data(is_train=False)
for idx, (data, label) in enumerate(tqdm(iterator)):
  batch_size = data.shape[0] # input batch size

  snn_layer.init_v_i(data)
  fea = snn_layer.run(data)

  test_X[start_save_idx: start_save_idx + batch_size, :] = fea
  test_Y[start_save_idx: start_save_idx + batch_size] = label
  start_save_idx += batch_size

print("finishing getting test data ...")

clf = LinearSVC(random_state=0, tol=1e-5, max_iter=10000)
clf.fit(train_X, train_Y)

predicated_labels = clf.predict(test_X)

acc = accuracy_score(test_Y, predicated_labels)
print("Accuracy: {}".format(acc))